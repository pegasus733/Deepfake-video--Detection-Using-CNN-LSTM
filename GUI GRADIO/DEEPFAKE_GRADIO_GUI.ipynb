{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDymX0TlkapO",
        "outputId": "64b7b7d8-0752-4c43-a386-64759524f273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install gradio opencv-python-headless pillow\n",
        "\n",
        "# Import all libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnext50_32x4d, ResNeXt50_32X4D_Weights\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Define your model architecture\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "class DeepfakeLSTMModel(nn.Module):\n",
        "    def __init__(self, hidden_size=HIDDEN_SIZE, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.backbone = resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=in_features,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, c, h, w = x.size()\n",
        "        x = x.view(batch_size * seq_len, c, h, w)\n",
        "        features = self.backbone(x)\n",
        "        features = features.view(batch_size, seq_len, -1)\n",
        "        lstm_out, _ = self.lstm(features)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Load model\n",
        "print(\"üîß Loading model...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = DeepfakeLSTMModel()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/deepfake_lstm_trained.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Extract frames function with preview images\n",
        "def extract_frames_with_preview(video_path, num_frames=10):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    if total_frames == 0:\n",
        "        cap.release()\n",
        "        return None, []\n",
        "\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    frames = []\n",
        "    preview_images = []\n",
        "\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Save for preview (original size)\n",
        "            preview_images.append(Image.fromarray(frame_rgb))\n",
        "\n",
        "            # Process for model\n",
        "            frame_tensor = transform(frame_rgb)\n",
        "            frames.append(frame_tensor)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        return None, []\n",
        "\n",
        "    frames_tensor = torch.stack(frames).unsqueeze(0)\n",
        "    return frames_tensor, preview_images\n",
        "\n",
        "# Create a grid of frames for display\n",
        "def create_frame_grid(images, max_display=6):\n",
        "    \"\"\"Create a grid of extracted frames\"\"\"\n",
        "    if not images:\n",
        "        return None\n",
        "\n",
        "    # Take only first max_display frames\n",
        "    display_images = images[:max_display]\n",
        "\n",
        "    # Calculate grid dimensions\n",
        "    n = len(display_images)\n",
        "    cols = 3\n",
        "    rows = (n + cols - 1) // cols\n",
        "\n",
        "    # Get individual image size\n",
        "    img_width, img_height = display_images[0].size\n",
        "\n",
        "    # Create grid\n",
        "    grid_width = img_width * cols + 20 * (cols - 1)\n",
        "    grid_height = img_height * rows + 20 * (rows - 1)\n",
        "    grid = Image.new('RGB', (grid_width, grid_height), color=(30, 30, 30))\n",
        "\n",
        "    # Paste images\n",
        "    for idx, img in enumerate(display_images):\n",
        "        row = idx // cols\n",
        "        col = idx % cols\n",
        "        x = col * (img_width + 20)\n",
        "        y = row * (img_height + 20)\n",
        "        grid.paste(img, (x, y))\n",
        "\n",
        "    return grid\n",
        "\n",
        "# Prediction function\n",
        "def predict_deepfake(video_file, sequence_length):\n",
        "    if video_file is None:\n",
        "        return \"‚ùå Please upload a video file\", None, None, None\n",
        "\n",
        "    try:\n",
        "        print(f\"üìπ Processing video: {video_file}\")\n",
        "        print(f\"üé¨ Sequence length: {sequence_length}\")\n",
        "\n",
        "        # Extract frames with preview\n",
        "        print(\"‚è≥ Extracting frames...\")\n",
        "        frames, preview_images = extract_frames_with_preview(video_file, num_frames=sequence_length)\n",
        "\n",
        "        if frames is None:\n",
        "            return \"‚ùå Failed to extract frames from video\", None, None, None\n",
        "\n",
        "        # Create frame grid for display\n",
        "        frame_grid = create_frame_grid(preview_images, max_display=6)\n",
        "\n",
        "        # Make prediction\n",
        "        print(\"ü§ñ Running model prediction...\")\n",
        "        with torch.no_grad():\n",
        "            frames = frames.to(device)\n",
        "            outputs = model(frames)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            confidence, predicted = torch.max(probabilities, 1)\n",
        "\n",
        "            prediction = 'FAKE' if predicted.item() == 0 else 'REAL'\n",
        "            confidence_score = confidence.item()\n",
        "\n",
        "        print(f\"‚úÖ Prediction: {prediction} ({confidence_score:.2%})\")\n",
        "\n",
        "        # Format results\n",
        "        if prediction == 'REAL':\n",
        "            result_text = f\"‚úÖ AUTHENTIC VIDEO\\n\\nConfidence: {confidence_score*100:.1f}%\"\n",
        "        else:\n",
        "            result_text = f\"‚ùå DEEPFAKE DETECTED\\n\\nConfidence: {confidence_score*100:.1f}%\"\n",
        "\n",
        "        # Create confidence breakdown\n",
        "        fake_conf = probabilities[0][0].item()\n",
        "        real_conf = probabilities[0][1].item()\n",
        "\n",
        "        confidence_dict = {\n",
        "            \"Fake\": fake_conf,\n",
        "            \"Real\": real_conf\n",
        "        }\n",
        "\n",
        "        details = f\"\"\"\n",
        "## üìä Analysis Details\n",
        "\n",
        "- **Prediction:** {prediction}\n",
        "- **Confidence:** {confidence_score*100:.1f}%\n",
        "- **Frames Analyzed:** {sequence_length}\n",
        "- **Model:** LSTM + ResNeXt50\n",
        "- **Device:** {device}\n",
        "\n",
        "### üîç Probability Breakdown:\n",
        "- **Fake:** {fake_conf*100:.1f}%\n",
        "- **Real:** {real_conf*100:.1f}%\n",
        "\n",
        "### üé¨ Frame Extraction:\n",
        "The model analyzed {len(preview_images)} frames evenly distributed throughout the video.\n",
        "Above you can see a preview of the first 6 frames that were processed.\n",
        "        \"\"\"\n",
        "\n",
        "        return result_text, confidence_dict, details, frame_grid\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"‚ùå Error: {str(e)}\", None, None, None\n",
        "\n",
        "# Create Gradio Interface\n",
        "print(\"üé® Creating Gradio interface...\")\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"purple\")) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üé¨ Deepfake Detector\n",
        "    ### Advanced LSTM-based Video Analysis\n",
        "    Upload a video file to analyze it for deepfake detection using ResNeXt50 + LSTM model.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Simple file upload\n",
        "            video_input = gr.File(\n",
        "                label=\"üìπ Upload Video File\",\n",
        "                file_types=[\"video\"],\n",
        "                type=\"filepath\"\n",
        "            )\n",
        "\n",
        "            sequence_slider = gr.Slider(\n",
        "                minimum=5,\n",
        "                maximum=30,\n",
        "                value=10,\n",
        "                step=1,\n",
        "                label=\"üé¨ Sequence Length (Number of frames to analyze)\",\n",
        "                info=\"Higher values = more accurate but slower\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üöÄ Analyze Video\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            result_text = gr.Textbox(label=\"üéØ Result\", lines=3, show_label=True)\n",
        "            confidence_plot = gr.Label(label=\"üìä Confidence Scores\", num_top_classes=2)\n",
        "\n",
        "    # Frame preview section\n",
        "    with gr.Row():\n",
        "        frame_gallery = gr.Image(label=\"üéûÔ∏è Extracted Frames Preview (First 6)\", show_label=True)\n",
        "\n",
        "    # Details section\n",
        "    with gr.Row():\n",
        "        details_text = gr.Markdown(label=\"üìã Analysis Details\")\n",
        "\n",
        "    gr.Markdown(f\"\"\"\n",
        "    ---\n",
        "    ### ‚ÑπÔ∏è Model Information\n",
        "    - **Architecture:** LSTM + CNN\n",
        "    - **Hidden Size:** 256\n",
        "    - **Classes:** Real / Fake\n",
        "    - **Running on:** {device}\n",
        "\n",
        "    ### üìù How to use:\n",
        "    1. Click \"Upload Video File\" and select your video\n",
        "    2. Adjust sequence length (default: 10 frames)\n",
        "    3. Click \"üöÄ Analyze Video\"\n",
        "    4. Wait 30-60 seconds for results\n",
        "    5. View extracted frames to see what the model analyzed\n",
        "\n",
        "    ### ‚ö° Tips:\n",
        "    - Supported formats: MP4, AVI, MOV, MKV\n",
        "    - Longer videos may take more time\n",
        "    - Higher sequence length = more accurate but slower\n",
        "    - Frames are extracted evenly throughout the video\n",
        "    \"\"\")\n",
        "\n",
        "    # Connect the analyze button\n",
        "    analyze_btn.click(\n",
        "        fn=predict_deepfake,\n",
        "        inputs=[video_input, sequence_slider],\n",
        "        outputs=[result_text, confidence_plot, details_text, frame_gallery]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ Launching Gradio Interface...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EgPQo-Ulrpqd",
        "outputId": "4bdc00d3-ecd4-4204-9b33-f83446cc28e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "üîß Loading model...\n",
            "Using device: cuda\n",
            "‚úÖ Model loaded successfully!\n",
            "üé® Creating Gradio interface...\n",
            "\n",
            "============================================================\n",
            "üöÄ Launching Gradio Interface...\n",
            "============================================================\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://917b986bb86b4d5d16.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://917b986bb86b4d5d16.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1134, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 125, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 111, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 391, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 290, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7c60cffdf260 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìπ Processing video: /tmp/gradio/4532c47fc1809a5f7f1b835b880845818894c13d912498135e4ca1916d3c384a/00066.mp4\n",
            "üé¨ Sequence length: 10\n",
            "‚è≥ Extracting frames...\n",
            "ü§ñ Running model prediction...\n",
            "‚úÖ Prediction: REAL (97.07%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1134, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 113, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 882, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 125, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 111, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 391, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 290, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1671, in get_upload_progress\n",
            "    await asyncio.wait_for(\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
            "    return await fut\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 528, in is_tracked\n",
            "    return await self._signals[upload_id].wait()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
            "    fut = self._get_loop().create_future()\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
            "    raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
            "RuntimeError: <asyncio.locks.Event object at 0x7c60cffdf260 [unset]> is bound to a different event loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìπ Processing video: /tmp/gradio/2b1a33187be5cb30ab0491868ee2d5f32253903592d4020aa48c7f3cbc9c2055/id0_id2_0002.mp4\n",
            "üé¨ Sequence length: 10\n",
            "‚è≥ Extracting frames...\n",
            "ü§ñ Running model prediction...\n",
            "‚úÖ Prediction: FAKE (71.74%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
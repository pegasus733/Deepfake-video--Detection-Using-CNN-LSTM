{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqlrM3SV4M8T",
        "outputId": "7cff2347-48ec-479c-ed8d-9d962ac10f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9R-Hn0u5CS5",
        "outputId": "8bbf6e1b-dc3c-41c3-f4f6-ca8a40b48b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: Imports, paths, hyperparameters\n",
        "\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnext50_32x4d, ResNeXt50_32X4D_Weights\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Paths (adjust only if your paths are different) ----\n",
        "BASE_DIR = \"/content/drive/MyDrive/FF_REAL_Face_only_data\"\n",
        "METADATA_JSON = \"/content/drive/MyDrive/metadata.json\"\n",
        "\n",
        "# ---- Hyperparameters ----\n",
        "SEQUENCE_LENGTH = 10      # frames per video\n",
        "IM_SIZE = 112\n",
        "BATCH_SIZE = 4\n",
        "NUM_WORKERS = 0           # keep 0 on Colab\n",
        "NUM_EPOCHS = 10\n",
        "LR = 1e-4\n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "# ---- Device ----\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_RkW4FJ5KTK",
        "outputId": "0706aee3-1451-4abd-a7b1-19347503935d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total entries in metadata: 200\n",
            "Total video files found: 200\n",
            "Videos with labels: 200\n",
            "Label counts: Counter({'REAL': 100, 'FAKE': 100})\n",
            "\n",
            "Train size: 160  Valid size: 40\n",
            "Train label counts: Counter({0: 80, 1: 80})\n",
            "Valid label counts: Counter({0: 20, 1: 20})\n",
            "Overlap between train and valid filenames (should be empty): set()\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: Load metadata + build labels + split\n",
        "\n",
        "# 1) Load metadata.json -> labels_map\n",
        "with open(METADATA_JSON, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "labels_map = {\n",
        "    os.path.basename(k): str(v[\"label\"]).upper()\n",
        "    for k, v in meta.items()\n",
        "}\n",
        "\n",
        "print(\"Total entries in metadata:\", len(labels_map))\n",
        "\n",
        "# 2) Collect all video files that actually exist\n",
        "video_files = sorted(glob.glob(os.path.join(BASE_DIR, \"*.mp4\")))\n",
        "print(\"Total video files found:\", len(video_files))\n",
        "\n",
        "# 3) Filter to only videos that have a label in metadata\n",
        "valid_video_files = []\n",
        "valid_labels = []\n",
        "\n",
        "for vp in video_files:\n",
        "    fname = os.path.basename(vp)\n",
        "    if fname in labels_map:\n",
        "        valid_video_files.append(vp)\n",
        "        valid_labels.append(labels_map[fname])\n",
        "\n",
        "print(\"Videos with labels:\", len(valid_video_files))\n",
        "print(\"Label counts:\", Counter(valid_labels))\n",
        "\n",
        "# 4) Convert labels to numeric (FAKE=0, REAL=1)\n",
        "label_to_num = {\"FAKE\": 0, \"REAL\": 1}\n",
        "numeric_labels = [label_to_num[lbl] for lbl in valid_labels]\n",
        "\n",
        "# 5) Stratified split\n",
        "indices = list(range(len(valid_video_files)))\n",
        "\n",
        "train_idx, valid_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.2,\n",
        "    stratify=numeric_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_videos = [valid_video_files[i] for i in train_idx]\n",
        "valid_videos = [valid_video_files[i] for i in valid_idx]\n",
        "\n",
        "train_labels = [numeric_labels[i] for i in train_idx]\n",
        "valid_labels_num = [numeric_labels[i] for i in valid_idx]\n",
        "\n",
        "print(\"\\nTrain size:\", len(train_videos), \" Valid size:\", len(valid_videos))\n",
        "print(\"Train label counts:\", Counter(train_labels))\n",
        "print(\"Valid label counts:\", Counter(valid_labels_num))\n",
        "\n",
        "# sanity: no overlap\n",
        "overlap = set(os.path.basename(p) for p in train_videos) & set(os.path.basename(p) for p in valid_videos)\n",
        "print(\"Overlap between train and valid filenames (should be empty):\", overlap)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FWCgPlM5PHz"
      },
      "outputs": [],
      "source": [
        "# CELL 3: Dataset class (for LSTM)\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "class VideoDatasetLSTM(Dataset):\n",
        "    def __init__(self, video_paths, labels, sequence_length=SEQUENCE_LENGTH, transform=None):\n",
        "        \"\"\"\n",
        "        video_paths: list of full paths\n",
        "        labels: list of ints (0=FAKE, 1=REAL)\n",
        "        \"\"\"\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.seq_len = sequence_length\n",
        "        self.transform = transform if transform is not None else train_transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        try:\n",
        "            while len(frames) < self.seq_len:\n",
        "                ok, frame = cap.read()\n",
        "                if not ok:\n",
        "                    break\n",
        "                # BGR -> RGB\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = self.transform(frame)   # (3,H,W)\n",
        "                frames.append(frame)\n",
        "        finally:\n",
        "            cap.release()\n",
        "\n",
        "        # If not enough frames, pad with last frame\n",
        "        if len(frames) == 0:\n",
        "            # completely broken video: fill zeros\n",
        "            frame = torch.zeros(3, IM_SIZE, IM_SIZE)\n",
        "            frames = [frame] * self.seq_len\n",
        "        elif len(frames) < self.seq_len:\n",
        "            last = frames[-1]\n",
        "            while len(frames) < self.seq_len:\n",
        "                frames.append(last.clone())\n",
        "\n",
        "        frames = torch.stack(frames)[:self.seq_len]  # (T,3,H,W)\n",
        "\n",
        "        return frames, torch.tensor(label, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUGOiXN85T8e",
        "outputId": "b0603174-0d14-4c50-e6e6-297c87bf693d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 40  Valid batches: 10\n",
            "Sample frames shape: torch.Size([10, 3, 112, 112])\n",
            "Sample numeric label: 0\n",
            "From labels_map: FAKE\n"
          ]
        }
      ],
      "source": [
        "# CELL 4: Dataloaders + sanity check\n",
        "\n",
        "train_dataset = VideoDatasetLSTM(train_videos, train_labels, sequence_length=SEQUENCE_LENGTH, transform=train_transforms)\n",
        "valid_dataset = VideoDatasetLSTM(valid_videos, valid_labels_num, sequence_length=SEQUENCE_LENGTH, transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader), \" Valid batches:\", len(valid_loader))\n",
        "\n",
        "# check one sample\n",
        "sample_frames, sample_label = train_dataset[0]\n",
        "print(\"Sample frames shape:\", sample_frames.shape)  # expected (T,3,112,112)\n",
        "print(\"Sample numeric label:\", sample_label.item())\n",
        "print(\"From labels_map:\", labels_map.get(os.path.basename(train_videos[0])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsRtKpQC5ZMK",
        "outputId": "83255688-4c34-466c-ebca-a26cd8d80589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95.8M/95.8M [00:01<00:00, 81.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model on cuda\n"
          ]
        }
      ],
      "source": [
        "# CELL 5: Define LSTM model\n",
        "\n",
        "class DeepfakeLSTMModel(nn.Module):\n",
        "    def __init__(self, hidden_size=HIDDEN_SIZE, num_layers=1, num_classes=2):\n",
        "        super().__init__()\n",
        "        # CNN backbone\n",
        "        self.backbone = resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()  # remove final FC\n",
        "\n",
        "        # LSTM on top of frame features\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=in_features,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # final classifier\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, 3, H, W)\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        x = x.view(B*T, C, H, W)          # (B*T,3,H,W)\n",
        "        feats = self.backbone(x)          # (B*T, 2048)\n",
        "\n",
        "        feats = feats.view(B, T, -1)      # (B, T, 2048)\n",
        "        lstm_out, _ = self.lstm(feats)    # (B, T, hidden)\n",
        "        last_out = lstm_out[:, -1, :]     # (B, hidden)\n",
        "\n",
        "        logits = self.fc(last_out)        # (B, 2)\n",
        "        return logits\n",
        "\n",
        "model = DeepfakeLSTMModel().to(device)\n",
        "print(\"Model on\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxkN62ov5fsF",
        "outputId": "6b8c9c0f-cebe-4245-bbaa-3fe40a71b31b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/10] Train Loss: 0.1832 Acc: 93.75% | Val Loss: 0.0798 Acc: 100.00%\n",
            "[Epoch 2/10] Train Loss: 0.2050 Acc: 93.12% | Val Loss: 0.1644 Acc: 92.50%\n",
            "[Epoch 3/10] Train Loss: 0.0521 Acc: 98.12% | Val Loss: 0.1202 Acc: 95.00%\n",
            "[Epoch 4/10] Train Loss: 0.1276 Acc: 96.88% | Val Loss: 0.1432 Acc: 95.00%\n",
            "[Epoch 5/10] Train Loss: 0.0443 Acc: 98.75% | Val Loss: 0.0414 Acc: 97.50%\n",
            "[Epoch 6/10] Train Loss: 0.0773 Acc: 97.50% | Val Loss: 0.0459 Acc: 97.50%\n",
            "[Epoch 7/10] Train Loss: 0.0618 Acc: 97.50% | Val Loss: 0.1941 Acc: 87.50%\n",
            "[Epoch 8/10] Train Loss: 0.0393 Acc: 98.12% | Val Loss: 0.2115 Acc: 90.00%\n",
            "[Epoch 9/10] Train Loss: 0.0690 Acc: 97.50% | Val Loss: 0.1978 Acc: 92.50%\n",
            "[Epoch 10/10] Train Loss: 0.0803 Acc: 97.50% | Val Loss: 0.4327 Acc: 85.00%\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# CELL 6: Train LSTM model\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for frames, labels in loader:\n",
        "        frames = frames.to(device)      # (B,T,3,112,112)\n",
        "        labels = labels.to(device)      # (B,)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(frames)         # (B,2)\n",
        "        loss = criterion(outputs, labels)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total * 100.0\n",
        "    return avg_loss, acc\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc = run_epoch(valid_loader, train=False)\n",
        "    print(f\"[Epoch {epoch}/{NUM_EPOCHS}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecLtsDNA6G81",
        "outputId": "b519cf9d-dd5a-47e9-f057-1f2995e616d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_true counts: Counter({'FAKE': 20, 'REAL': 20})\n",
            "y_pred counts: Counter({'FAKE': 26, 'REAL': 14})\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred): ['REAL', 'FAKE']\n",
            "[[14  6]\n",
            " [ 0 20]]\n",
            "\n",
            "Accuracy: 85.00%\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        REAL       1.00      0.70      0.82        20\n",
            "        FAKE       0.77      1.00      0.87        20\n",
            "\n",
            "    accuracy                           0.85        40\n",
            "   macro avg       0.88      0.85      0.85        40\n",
            "weighted avg       0.88      0.85      0.85        40\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CELL 7: Evaluate on validation set\n",
        "\n",
        "model.eval()\n",
        "all_true = []\n",
        "all_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for frames, labels in valid_loader:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(frames)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_true.extend(labels.cpu().numpy().tolist())\n",
        "        all_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "num_to_str = {0: \"FAKE\", 1: \"REAL\"}\n",
        "true_str = [num_to_str[x] for x in all_true]\n",
        "pred_str = [num_to_str[x] for x in all_pred]\n",
        "\n",
        "print(\"y_true counts:\", Counter(true_str))\n",
        "print(\"y_pred counts:\", Counter(pred_str))\n",
        "\n",
        "labels_order = [\"REAL\", \"FAKE\"]\n",
        "cm = confusion_matrix(true_str, pred_str, labels=labels_order)\n",
        "print(\"\\nConfusion matrix (rows=true, cols=pred):\", labels_order)\n",
        "print(cm)\n",
        "\n",
        "acc = accuracy_score(true_str, pred_str) * 100.0\n",
        "print(f\"\\nAccuracy: {acc:.2f}%\")\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(true_str, pred_str, labels=labels_order, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmI9cWrK6P__",
        "outputId": "8952dd66-5fb2-44d2-8b14-d25067a7b08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to: /content/drive/MyDrive/deepfake_lstm_trained.pth\n"
          ]
        }
      ],
      "source": [
        "# CELL 8: Save trained model\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/deepfake_lstm_trained.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Saved model to:\", save_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}